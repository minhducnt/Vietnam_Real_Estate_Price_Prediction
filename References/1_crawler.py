# -*- coding: utf-8 -*-
"""Nhom05-Crawler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15L7OH_h94z09iuYg0LAw6K7SLBhL9_q3

# **ĐỀ TÀI: ỨNG DỤNG BIG DATA XÂY DỰNG MODEL DỰ BÁO GIÁ NHÀ ĐẤT CỦA VIỆT NAM VỚI BỘ DỮ LIỆU THU THẬP ĐƯỢC TỪ nhadat.cafeland.vn**

# Cài đặt môi trường
"""

# Cài đặt thư viện Selenium
!pip install selenium

# Cài đặt trình duyệt Chromium và Chromedriver – trình điều khiển" giúp Selenium giao tiếp với Chromium
!apt-get update
!apt-get install chromium-driver
!apt-get install -y chromium-browser
!apt install chromium-chromedriver

# Import thư viện cần thiết
from selenium import webdriver
from selenium.webdriver.common.by import By

# Import thư viện cần thiết
import time
import random
import csv
import pandas as pd

"""# Thu thập dữ liệu"""

# Cấu hình trình duyệt chạy ngầm (headless) để tối ưu tốc độ và tránh lỗi hệ thống
options = webdriver.ChromeOptions()
options.add_argument("--verbose")
options.add_argument('--no-sandbox')
options.add_argument('--headless') # chạy không hiển thị trình duyệt
options.add_argument('--disable-gpu')
options.add_argument("--window-size=1920, 1200")
options.add_argument('--disable-dev-shm-usage') # hạn chế lỗi bộ nhớ trong Docker/Colab
options.add_experimental_option('excludeSwitches', ['enable-logging'])

# Khởi tạo đối tượng driver, là cửa sổ trình duyệt ảo sẽ được Selenium điều khiển.
driver = webdriver.Chrome(options=options)

# Chạy lại cell này nếu bị lỗi ở dưới

"""## Thu thập dữ liệu link các bài đăng"""

# Mở trang danh sách bất động sản bán
url = "https://nhadat.cafeland.vn/nha-dat-ban/"
print(f"Đang crawl trang: {url}")
driver.get(url)

# Chờ trang load hoàn chỉnh
time.sleep(3)

# Thu thập link các bài đăng
article_link = []

# Lặp qua từng trang kết quả
for page in range(1, 2):
    url = f"https://nhadat.cafeland.vn/nha-dat-ban/?page={page}"
    print(f"Đang crawl trang: {page}")
    driver.get(url)

    # Chờ trang load hoàn chỉnh
    time.sleep(random.uniform(3,6)) # Sử dụng random để mô phỏng người dùng thật, tránh bị chặn

    try:
      # Tìm tất cả phần tử chứa bài đăng
        posts = driver.find_elements(By.XPATH, "//div[contains(@class, 'property-list')]/div[contains(@class, 'row-item')]")

      # Trích xuất liên kết bài đăng từ thẻ <a>, sau đó lưu vào danh sách article_link
        for post in posts:
            try:
                link_elem = post.find_elements(By.XPATH, ".//a")

                link = link_elem[0].get_attribute("href") if link_elem else ""

                article_link.append({
                  "link": link
                })

            except Exception as e:
                print("Lỗi khi xử lý 1 bài:", e)

    except Exception as e:
        print("Lỗi khi truy cập danh sách bài:", e)

# Đóng trình duyệt
driver.quit()

# In kết quả
print(f"Thu được tổng cộng {len(article_link)} link bài đăng.")

"""## Thu thập dữ liệu bất động sản từ link"""

options = webdriver.ChromeOptions()
options.add_argument("--verbose")
options.add_argument('--no-sandbox')
options.add_argument('--headless')
options.add_argument('--disable-gpu')
options.add_argument("--window-size=1920, 1200")
options.add_argument('--disable-dev-shm-usage')
options.add_experimental_option('excludeSwitches', ['enable-logging'])

driver = webdriver.Chrome(options=options)

# Tạo hàm tiện ích để lấy nội dung từ XPath. Nếu phần tử không tồn tại, trả về chuỗi rỗng thay vì lỗi
def safe_get_text(xpath):
    try:
        return driver.find_element(By.XPATH, xpath).text
    except:
        return ""

# Thu thập dữ liệu
data = []

# Lặp qua từng liên kết bài đăng, truy cập và chờ trang tải
for idx, article in enumerate(article_link):
    url = article['link']
    print(f"[{idx+1}/{len(article_link)}] Đang crawl chi tiết: {url}")

    try:
        driver.get(url)
        time.sleep(3)

        # Lấy từng trường thông tin
        item = {
            "link": url,
            "title": safe_get_text("//h1[contains(@class, 'head-title')]"),
            "location": safe_get_text("//div[contains(@style, 'float:left; width:87%; padding-left: 5px;')]"),
            "price": safe_get_text("(//div[contains(@class, 'col-item')]/div[contains(@class, 'infor-note')])[2]"),
            "area": safe_get_text("(//div[contains(@class, 'col-item')]/div[contains(@class, 'infor-data')])[2]"),
            "category": safe_get_text("//div[contains(@class, 'opt-mattien')]/span[contains(@class, 'value-item')]"),
            "direction": safe_get_text("//div[contains(@class, 'opt-huongnha')]/span[contains(@class, 'value-item')]"),
            "floor_num": safe_get_text("//div[contains(@class, 'opt-sotang')]/span[contains(@class, 'value-item')]"),
            "toilet_num": safe_get_text("//div[contains(@class, 'opt-sotoilet')]/span[contains(@class, 'value-item')]"),
            "street": safe_get_text("//div[contains(@class, 'opt-duong')]/span[contains(@class, 'value-item')]"),
            "livingroom_num": safe_get_text("//div[contains(@class, 'opt-bancong')]/span[contains(@class, 'value-item')]"),
            "bedroom_num": safe_get_text("//div[contains(@class, 'opt-sopngu')]/span[contains(@class, 'value-item')]"),
            "liability": safe_get_text("//div[contains(@class, 'opt-phaply')]/span[contains(@class, 'value-item')]"),
        }

        # Lưu vào data
        data.append(item)

    except Exception as e:
        print("Lỗi khi crawl chi tiết:", e)

# Đóng trình duyệt
driver.quit()

# In thử kết quả
print(f"\nĐã crawl chi tiết {len(data)} bài.")
for d in data[:3]:
    print(d)

"""# Lưu dữ liệu"""

# Tạo DataFrame từ danh sách data
df = pd.DataFrame(data)

# Lưu ra file CSV với encoding phù hợp cho tiếng Việt
df.to_csv("batdongsan.csv", index=False, encoding="utf-8-sig")

print("Đã lưu dữ liệu ra file batdongsan.csv")

# Tải file về máy tính người dùng từ Google Colab
from google.colab import files
files.download("batdongsan.csv")